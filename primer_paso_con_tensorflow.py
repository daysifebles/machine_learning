# -*- coding: utf-8 -*-
"""Primer_paso_con_TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Syg6p3MuRMuPlyFGqj8n1VQ6ldlkdAvx

# Primeros pasos con TensorFlow
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function

import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow.python.data import Dataset

tf.logging.set_verbosity(tf.logging.ERROR)
pd.options.display.max_rows = 10
pd.options.display.float_format = '{:.1f}'.format

"""# Cargar los datos

El conjunto de datos de viviendas de California contiene datos extraídos del censo nacional de 1990. La siguiente tabla proporciona descripciones, rangos de datos y tipos de datos para cada atributo del conjunto de datos.

| Título de la columna |	Descripción	|Rango*	| Tipo de dato |
|:--------------------:|:------------|:-----|:------------|
| longitude |	Una medida de cuán al oeste se encuentra una casa,<br /> una valor mayor significa más al oeste | Valores de longitud desde -180 hasta +180 <br /> Mínimo del conjunto de datos: -124.3 <br /> Máximo del conjunto de datos: -114.3 | float64 |
|latitude |	Una medida de cuán al norte se encuentra una casa, <br /> una valor mayor significa más al norte | Valores de latitud desde -90 hasta +90 <br /> Mínimo del conjunto de datos: 32.5 <br /> Máximo del conjunto de datos: 42.5 | float64 |
| housingMedianAge | Antigüedad mediana de una casa en una manzana,<br />  una cifra menor significa una construcción más nueva |	Mínimo del conjunto de datos: 1.0 <br /> Máximo del conjunto de datos: 52.0 | float64 |
| totalRooms |	Cantidad total de habitaciones en una manzana | Mínimo del conjunto de datos: 2.0 <br /> Máximo del conjunto de datos: 37937.0 | float64 |
| totalBedrooms |	Cantidad total de camas en una manzana | Mínimo del conjunto de datos: 1.0 <br /> Máximo del conjunto de datos: 6445.0 | float64 |
| population |	Cantidad total de residentes en una manzana	| Mínimo del conjunto de datos: 3.0 <br /> Máximo del conjunto de datos: 35682.0 | float64 |
| households |	Cantidad total de grupos familiares (conjunto <br />  de personas que reside en una misma casa) <br />  en una manzana	| Mínimo del conjunto de datos: 1.0 <br />  Máximo del conjunto de datos: 6082.0 | float64 | 
| medianIncome |	Ingreso mediano de los grupos familiares <br />  de una manzana (medido en decenas de miles <br />  de dólares estadounidenses)	| Mínimo del conjunto de datos: 0.5 <br /> Máximo del conjunto de datos: 15.0 | float64 |
| medianHouseValue |	Valor mediano de una casa para grupos <br />  familiares en una misma manzana <br /> (medido en dólares estadounidenses)	| Mínimo del conjunto de datos: 14999.0 <br />  Máximo del conjunto de datos: 500001.0 | fl |


**Referencia** : 
Pace, R. Kelley, y Ronald Barry, "Sparse Spatial Autoregressions," Statistics and Probability Letters, volumen 33, número 3, 5 de mayo de 1997, pp. 291-297.
"""

california_housing_dataframe = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv", sep=",")
california_housing_dataframe

#Generamos una permutación aleatoria del indice para reordenar los datos de forma aleatoria
california_housing_dataframe = california_housing_dataframe.reindex(
    np.random.permutation(california_housing_dataframe.index))

#A continuación se reescalara la variable median_house_value en unidades de 1000
california_housing_dataframe["median_house_value"] /= 1000.0
california_housing_dataframe

"""# Examinación de la Data

El comando que usaremos para obtener un resumen detallado de las características de cada variable de la data es `describe()` de la libreria `pandas`. 

Esta nos da: cantidad total, media, desviación estándar, valor mínimo, cuantil al 25\%, 50\% y 75\%, valor máximo.
"""

california_housing_dataframe.describe()

"""# Construcción del primer modelo

Trataremos de predecir el `median_house_value`, éste será nuestra etiqueta. Usaremos `total_rooms` como la característica o variable de entrada.

Se usara ***LinearRegressor*** proporcionada por ***TensorFlow Estimator API***.

## Paso I: Definir las variables y configurar las columnas de las variables

Necesitamos especificar que tipo de data contiene cada variable. Hay dos tipos de datos que se usarán en este y los ejercicios futuros:

- **Dato categórico:** Este dato es textual. En este ejercicios la data no contiene datos categóricos, pero un ejemplo sería ver el estilo de casa, las palabras en un anuncio inmobiliario.

- **Dato numérico:** Este dato es un número, entero o flotante, y los que se quieren tratar como números. Muchas veces se quiere tratar datos numéricos como categoricos, ejemplo el código postal.

En `TensorFlow` indicamos el tipo de variable usando una construcción llamada `feature column`, estas contienen solamente la descripción de la variable, no contienen los datos de las variables en si.

Como se dijo anteriormente solo usaremos una variable de entrada, `total_rooms`
"""

# Definición de la variable de entrada
my_feature= california_housing_dataframe[['total_rooms']]

# Configuración de la variable como numérica.
feature_columns = [tf.feature_column.numeric_column("total_rooms")]

#Este último comando transforma la data en un array de una dimensión 
#(una lista del total de número de habitaciones para cada bloque)

"""## Paso II : Definir el objetivo

Nuestros objetivo es la variable `median_house_value`.
"""

#Definición de la variable objetivo
targets = california_housing_dataframe['median_house_value']

"""## Paso III: Configurar el LinearRegressor

A continuación configuraremos el modelo de regresión lineal usando `LinearRegressor`. Entrenaremos el modelo usando `GradientDescentOptimizer` el cual implementa el descenso del gradiente estocástico de mini lote. El argumento `learning_rate` controla el tamaño de paso del gradiente.

**NOTA:**

Aplicaremos el recorte de gradiente, para optimizar mediante `clip_gradients_by_norm`. El recorte de gradiente garantiza que la magnitud de los gradientes no sea muy grande durante el entrenamiento, lo que puede provocar que el descenso del gradiente falle.
"""

# Usar el gradiente de descenso como un optimizador para entrenar el modelo.
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)

# Configuración del modelo de regresión linea con nuestra variable de entrada
# y optimizador . La tasa de aprendizaje que usaremos es 0.0000001
linear_regressor = tf.estimator.LinearRegressor(
    feature_columns=feature_columns,
    optimizer=my_optimizer
)

"""## Paso IV: Definir la función de entrada

Para importar nuestra data a LinearRegressor, necesitamos definir la función de entrada, la cual le indica a `TensorFlow` como procesar los datos, así como procesarlos, mezclarlos y repetirlos mediante el entrenamiento del modelo.

Primero, convertimos nuestra variable de entrada (`pandas`) en un dict de arrays (`NumPy`). Usaremos entonces `TensorFlow` dataset API para construir los datos objetivos desde nuestros datos, y luego dividir nuestros datos en lotes de tamaño `batch_size`, esto se repetira para el número especificado de épocas `num_epochs`.

**NOTA:**

Cando el valor de `num_epochs` no se específica, es decir se pasa a `repeat()` es `num_epochs=None` y se repite indefinidamente.

Si `shuffle` es `True`, entonces se barajará los datos y esto se pasará al modelo aleatoriamente durante en entrenamiento.

El argumento `buffer_size` específica el tamaño del conjunto de datos en el cual `shuffle` toma aleatoriamente la muestra.

Finalmente la función de entrada construida un iterador para el subconjunto de datos y retorna el siguiente lote de datos para LinearRegressor.
"""

def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):
    """Entrenamiento de un modelo de regresión lineal con una variable de entrada
  
    Argumentos:
      features: pandas DataFrame de variables
      targets: pandas DataFrame de variable objetivo
      batch_size: Tamaño del lote para ser pasado al modelo
      shuffle: True o False. Barajear los datos.
      num_epochs: Número de epocas para los cuales se deben repetir los datos.
               None = repetir indefinidamente
    Devuelve:
      Tuple de (features, labels) para el siguiente lote
    """
  
    # Convertir datos pandas a diccionario de np arrays.
    features = {key:np.array(value) for key,value in dict(features).items()}                                           
 
    # Construir un subconjunto de datos y configurar el lote y repetir.
    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit
    #construye un subconjunto de datos.
    ds = ds.batch(batch_size).repeat(num_epochs)
    
    # Barajear los datos, si se espeficica.
    if shuffle:
      ds = ds.shuffle(buffer_size=10000) #barajea aleatoriamente subconjunto de datos
                                         # de tamaño 10000
    
    # Devuelve el siguiente lote de datos.
    features, labels = ds.make_one_shot_iterator().get_next()
    return features, labels

"""## Paso V: Entrenar el modelo

Ahora entrenaremos el modelo usando `train()` en nuestro `linear_regressor`. Envolveremos la función `funcion_entrada` en un `lambda` al cual pasamos la variable de entrada y la variable objetivo como argumentos, para empezar utilizamos entrenamiento por 100 pasos.
"""

_ = linear_regressor.train(
    input_fn = lambda:my_input_fn(my_feature, targets),
    steps=100
)

"""## Paso VI: Evaluar el Modelo

Ahora realizaremos predicción en la data entrenada, para ver que tan bien se ajusta nuestro modelo durante el entrenamiento.

**NOTA:**

El error de entrenamiento nos  dice que tan bien se ajustan los datos de entrenamiento, pero esto no mide que tan bien el modelo se generaliza a los nuevos datos.
"""

# Creación de una función de entrada para predecir.
# Nota:Como solo hacemos una predicción para cada ejemplo, necesitamos
# repetir o need to repeat o barajar los datos.
prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)

# Realizar la predicción
predictions = linear_regressor.predict(input_fn=prediction_input_fn)

# Formatear predicciones como un NumPy array.
predictions = np.array([item['predictions'][0] for item in predictions])

# Imprimir el error cuadrático medio y la raiz del error cuadrático medio.
mean_squared_error = metrics.mean_squared_error(predictions, targets)
root_mean_squared_error = math.sqrt(mean_squared_error)
print("Mean Squared Error (on training data): %0.3f" % mean_squared_error)
print("Root Mean Squared Error (on training data): %0.3f" % root_mean_squared_error)

"""Esto es un buen modelo? Cómo sejuzgaria que tan grande es el error?

El error cuadrático medio (MSE) puede ser difícil de interpretar, frecuentemente se mira la raíz del error cuadrático medio (RMSE). Una buena propiedad de la raíz del error cuadrático medio es que se puede interpretar en la misma escala que la variable objetivo original.

Compararemos la raíz del error cuadrático medio con la diferencia del mínimo y máximo de nuestro objetivo.
"""

#Valor mínimo de la variable objetivo
min_house_value = california_housing_dataframe["median_house_value"].min()

#Valor máximo de la variable objetivo
max_house_value = california_housing_dataframe["median_house_value"].max()

#Diferencia entre el valor máximo y mínimo
min_max_difference = max_house_value - min_house_value

print("Mínimo de la variable objetivo: %0.3f" % min_house_value)
print("Máximo de la variable objetivo: %0.3f" % max_house_value)
print("Diferencia entre el mínimo y máximo: %0.3f" % min_max_difference)
print("Raíz del error cuadrático medio: %0.3f" % root_mean_squared_error)

"""El error abarca casi la mitad del rango de los valores de la variable objetivo. ¿Podemos hacerlo mejor?

Esta pregunta incomoda a todos los desarrolladores de modelos. Desarrollaremos algunas estrategias básicas para reducir el error del modelo.

La primera cosa que haremos es mirar que tan bien nuestras predicciones coinciden con nuestra variable objetivo, en terminos de resumen de estadísticas generales.
"""

#Construiremos un dataframe con las variables predictions y targets
calibration_data = pd.DataFrame()
calibration_data['predictions'] = pd.Series(predictions)
calibration_data["targets"] = pd.Series(targets)

#Resumen estadístico del dataframe
calibration_data.describe()

"""Podría esta información ser util. ¿Cómo comparamos el valor medio con el RMSE del modelo?

También visualizamos los datos y la línea que hemos aprendido. Recordemos que la línea de regresión con una sola variable característica puede ser dibujada como una línea sobre el mapeo de la entrada x y la salida y.

Primero, obtendremos una muestra aleatoria uniforme de los datos para poder hacer un diaframa de dispersión legible.
"""

sample = california_housing_dataframe.sample(n=300)
sample

"""Seguido, dibujaremos la línea aprendida, dibujando a partir del termino del sesgos y el peso de las caraterísticas, junto con el diagrama de dispersión. La línea se mostrará en rojo."""

# Obtenemos el mínimo y el máximo de los valores de total_rooms.
x_0 = sample["total_rooms"].min()
x_1 = sample["total_rooms"].max()

# Peso final y sesgo generado durante el entrenamiento ax+b=weight x + bias
weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]
weight
bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')
bias

# obtención de la predicción para los valores máximo y mínimo
y_0 = weight*x_0 + bias
y_1 = weight*x_1 + bias

# Gráfica de la linea de regresión que pasa por (x_0,y_0) y (x_1,y_1)
plt.plot([x_0,x_1],[y_0,y_1], c='r')
plt.ylabel('Y : median_house_value') # nombre del eje y
plt.xlabel('X : total_rooms')# nombre del eje x
plt.scatter(sample['total_rooms'],sample['median_house_value']) # puntos muestrales
plt.show()

"""Esta líneainicial se ve muy lejo de los datos. Podemos encontar una línea mejor para ajustar.

## Paso VII : Ajustar los hiperparametros del modelo.
"""

def train_model(learning_rate, steps, batch_size, input_feature="total_rooms"):
  """Trains a linear regression model of one feature.
  
  Args:
    learning_rate: A `float`, the learning rate.
    steps: A non-zero `int`, the total number of training steps. A training step
      consists of a forward and backward pass using a single batch.
    batch_size: A non-zero `int`, the batch size.
    input_feature: A `string` specifying a column from `california_housing_dataframe`
      to use as input feature.
  """
  
  periods = 10
  steps_per_period = steps / periods

  my_feature = input_feature
  my_feature_data = california_housing_dataframe[[my_feature]]
  my_label = "median_house_value"
  targets = california_housing_dataframe[my_label]

  # Create feature columns.
  feature_columns = [tf.feature_column.numeric_column(my_feature)]
  
  # Create input functions.
  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)
  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)
  
  # Create a linear regressor object.
  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
  linear_regressor = tf.estimator.LinearRegressor(
      feature_columns=feature_columns,
      optimizer=my_optimizer
  )

  # Set up to plot the state of our model's line each period.
  plt.figure(figsize=(15, 6))
  plt.subplot(1, 2, 1)
  plt.title("Learned Line by Period")
  plt.ylabel(my_label)
  plt.xlabel(my_feature)
  sample = california_housing_dataframe.sample(n=300)
  plt.scatter(sample[my_feature], sample[my_label])
  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]

  # Train the model, but do so inside a loop so that we can periodically assess
  # loss metrics.
  print("Training model...")
  print("RMSE (on training data):")
  root_mean_squared_errors = []
  for period in range (0, periods):
    # Train the model, starting from the prior state.
    linear_regressor.train(
        input_fn=training_input_fn,
        steps=steps_per_period
    )
    # Take a break and compute predictions.
    predictions = linear_regressor.predict(input_fn=prediction_input_fn)
    predictions = np.array([item['predictions'][0] for item in predictions])
    
    # Compute loss.
    root_mean_squared_error = math.sqrt(
        metrics.mean_squared_error(predictions, targets))
    # Occasionally print the current loss.
    print("  period %02d : %0.2f" % (period, root_mean_squared_error))
    # Add the loss metrics from this period to our list.
    root_mean_squared_errors.append(root_mean_squared_error)
    # Finally, track the weights and biases over time.
    # Apply some math to ensure that the data and line are plotted neatly.
    y_extents = np.array([0, sample[my_label].max()])
    
    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]
    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')

    x_extents = (y_extents - bias) / weight
    x_extents = np.maximum(np.minimum(x_extents,
                                      sample[my_feature].max()),
                           sample[my_feature].min())
    y_extents = weight * x_extents + bias
    plt.plot(x_extents, y_extents, color=colors[period]) 
  print("Model training finished.")

  # Output a graph of loss metrics over periods.
  plt.subplot(1, 2, 2)
  plt.ylabel('RMSE')
  plt.xlabel('Periods')
  plt.title("Root Mean Squared Error vs. Periods")
  plt.tight_layout()
  plt.plot(root_mean_squared_errors)

  # Output a table with calibration data.
  calibration_data = pd.DataFrame()
  calibration_data["predictions"] = pd.Series(predictions)
  calibration_data["targets"] = pd.Series(targets)
  display.display(calibration_data.describe())

  print("Final RMSE (on training data): %0.2f" % root_mean_squared_error)

train_model(
    learning_rate=0.00001,
    steps=100,
    batch_size=1
)

train_model(
    learning_rate=0.00002,
    steps=500,
    batch_size=5
)