# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xVEIpoTqnx9qLuB4t2AbzWFZOHIBmodD
"""

import numpy as np
import scipy as sc
import matplotlib.pyplot as plt

from sklearn.datasets import make_circles

# Se realizará una red neuronal para la clasificación de datos.

# crear el dataset
n = 500 #número de registro en los datos
p = 2 #característica del registro de los datos

# make_circles? #para obtener la documentación de la función
# esta función generara el conjunto de datos tanto X como Y.

X, Y = make_circles(n_samples=n, factor=0.5, noise =0.05)
Y = Y[:,np.newaxis]

#En la variable X guarda el conjunto de datos que se usan como parámetro de 
# entrada y en Y se guarda un vector binario.
#factor: indica la distancia entre los dos circulos
#X, Y = make_circles(n_samples=n, factor=0.9)
#X, Y = make_circles(n_samples=n, factor=0.3)
#noise: agregar ruido a los datos, para que sea un poco más complicado
# y se asemeje más a la realidad, dentro de lo que cabe en ese problema

#print(Y)
#plt.scatter(X[:,0],X[:,1])
#plt.show()

#Para mostrar los circulos por separado
plt.scatter(X[Y[:,0] == 0,0],X[ Y[:,0]==0 ,1], c='skyblue')
plt.scatter(X[Y[:,0] == 1,0],X[Y[:,0]==1 ,1], c='salmon')
plt.axis('equal') #mostrar ejes con la misma proporción
plt.show()

#Lo que buscamos con la red neuronal es que separe en dos nubes de puntos
# diferentes lo que se muestra en el último gráfico

# cada capa es un modulo, dentro de una misma capa se van a realizar
#las mismas operaciones.

#Vamos a crear una clase que se refiera a una capa
class neural_layer():
  def __init__(self, n_conn, n_neur, act_f):
    self.act_f = act_f 
    self.b = np.random.rand(1,n_neur) *2 -1 #parámetro de bayas, vector columna aleatorio (0,1), al multiplicar por 2 y restar 1 normalizamos la variable 
    self.w = np.random.rand(n_conn,n_neur) *2 -1 #matriz
  #n_conn: número de conecciones que entran a la capa de la capa anterior.
  #n_neur: número de neuronas que hay en nuestra capa.
  #act_f: función de activación que se esta usando en la capa

# funciones de activación

#función sigmoidal y su derivada
sigm = (lambda x: 1/(1+np.e**(-x)), 
        lambda x: x*(1-x))

#función relu
relu = lambda x: np.maximum(0,x)

_x = np.linspace(-5,5,100)
#plt.plot(_x,sigm[0](_x)) #gráfico de la función sigmoide
plt.plot(_x,sigm[1](_x)) #gráfico de la derivada

l0 = neural_leyer(p,4,sigm)
l1 = neural_leyer(4,8,sigm)
#...

#Topología de la red
#topology = [p,4,8,16,8,4,1]
#p : número de neuronas que tenga nuestra capa
#Los valores 4,8,16,8,4 corresponden al número de neuronas en cada capa
#El 1 es la última neurona que dará solo dos resultado, 0 y 1

#En este caso vamos a asumir que todas las capas tienen la misma función deactivación
def create_nn(topology, act_f):
    nn = []
    for l, layer in enumerate( topology[:-1]):
      nn.append(neural_layer(topology[l], topology[l+1], act_f))
    return nn

topology = [p,4,8,16,8,4,1]

neural_net = create_nn(topology,sigm)

#función de coste con su derivada
l2_cost = (lambda Yp, Yr: np.mean((Yp-Yr) ** 2),
          lambda Yp, Yr: (Yp-Yr) )

#función para entrenar la red
#lr : factor que se utiliza en el descenso del gradiente
def train( neural_net, X, Y, l2_cost, lr=0.5, train = True):
  out = [(None, X)]
  # Forward pass, utilizar el vector de entrada pasarlo capa por capa
  for l, layer in enumerate(neural_net):
    z = out[-1][1] @ neural_net[l].w + neural_net[l].b #z = X @ neural_net[l].W + neural_net[l].b
    a = neural_net[l].act_f[0](z)
    out.append((z,a))

  print(l2_cost[0](out[-1][1], Y)) # print(out[-1][1])

  if train:
    # Backward pass
    deltas = []

    for l in reversed(range(0,len(neural_net))):
      z = out[l+1][0]
      a = out[l+1][1]

      #print(a.shape)

      if l == len(neural_net)-1:
        #calcular delta última capa
        deltas.insert(0,l2_cost[1](a, Y) * neural_net[l].act_f[1](a))
      else:
        #calcular delta respecto a capa previa
        deltas.insert(0,deltas[0] @ _w.T * neural_net[l].act_f[1](a) )
    
      _w = neural_net[l].w

    # Gradient descent
    neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True)*lr
    neural_net[l].w = neural_net[l].w - out[l][1].T @ deltas[0]*lr

  return out[-1][1]

train(neural_net, X, Y, l2_cost,0.5)
print(' ')

# VISUALIZACIÓN Y TEST

import time
from IPython.display import clear_output

neural_n = create_nn(topology, sigm)

loss = []

for i in range(2500):
    
  # Entrenemos a la red!
  pY = train(neural_n, X, Y, l2_cost, lr=0.05)
  
  if i % 25 == 0:
    
    print(pY)
  
    loss.append(l2_cost[0](pY, Y))
  
    res = 50

    _x0 = np.linspace(-1.5, 1.5, res)
    _x1 = np.linspace(-1.5, 1.5, res)

    _Y = np.zeros((res, res))

    for i0, x0 in enumerate(_x0):
      for i1, x1 in enumerate(_x1):
        _Y[i0, i1] = train(neural_n, np.array([[x0, x1]]), Y, l2_cost, train=False)[0][0]    

    plt.pcolormesh(_x0, _x1, _Y, cmap="coolwarm")
    plt.axis("equal")

    plt.scatter(X[Y[:,0] == 0, 0], X[Y[:,0] == 0, 1], c="skyblue")
    plt.scatter(X[Y[:,0] == 1, 0], X[Y[:,0] == 1, 1], c="salmon")

    clear_output(wait=True)
    plt.show()
    plt.plot(range(len(loss)), loss)
    plt.show()
    time.sleep(0.5)

